{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: A Powerful Ensemble Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a type of ensemble learning method for classification and regression that consists of multiple decision trees. Each tree in the forest is generated using a random subset of the data, and the final prediction is made by averaging the predictions of all the trees. This method helps to reduce overfitting, which is a common problem with decision trees, and improves the overall performance of the model. Additionally, random forests can also be used to estimate feature importance, which can be useful for feature selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is an example of how to use the scikit-learn library to train a random forest classifier with grid search for hyperparameter tuning, and visualize the ROC curve."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate sample data for classification using the `make_classification` function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the parameters for grid search, including the number of trees in the forest (`n_estimators`), the maximum depth of each tree (`max_depth`), and the minimum number of samples required to split an internal node (`min_samples_split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters to be searched in the grid search\n",
    "param_grid = {'n_estimators': [10, 50, 100],\n",
    "              'max_depth': [None, 5, 10],\n",
    "              'min_samples_split': [2, 5, 10]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we initialize a random forest classifier object and use the `GridSearchCV` class from scikit-learn to perform the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get the best estimator using the best_estimator_ attribute of the grid_search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator \n",
    "best_clf = grid_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `predict_proba` method of the best estimator to get the predicted probability of positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted probability of positive class\n",
    "y_score = best_clf.predict_proba(X)[:, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the `roc_curve` function from the `sklearn.metrics` library to compute the false positive rate (FPR) and true positive rate (TPR) for the predicted scores. The area under the ROC curve (AUC) is also computed using the auc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y, y_score)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we use the `matplotlib.pyplot` library to plot the ROC curve by plotting the FPR and TPR against each other. The ROC curve is a useful tool for evaluating the performance of binary classifiers, as it allows to visualize the trade-off between the true positive rate and the false positive rate. A classifier with a higher AUC (area under the ROC curve) is generally considered to be a better classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, this code sample demonstrates how to use the scikit-learn library to train a random forest classifier with grid search for hyperparameter tuning, and how to visualize the ROC curve. The `RandomForestClassifier` class from the `sklearn.ensemble` library is used to train the random forest classifier, and the `GridSearchCV` class from the `sklearn.model_selection` library is used to perform the grid search. The sample data is generated using the `make_classification` function from the `sklearn.datasets` library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
