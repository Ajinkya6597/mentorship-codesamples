{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an open-source software library for gradient boosting on decision trees. It is designed for efficient and scalable handling of large datasets and is particularly useful for machine learning competitions and other high-performance machine learning tasks. It can be used as a standalone classifier, or as an advanced component in a larger machine learning pipeline. XGBoost is widely used in industry and academia due to its high performance and ease of use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost to train a classifier on a built-in dataset from scikit-learn, and plotting the training loss:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does the following:\n",
    "\n",
    "- Import the necessary libraries: xgboost, scikit-learn's load_iris function, train_test_split function and matplotlib for plotting\n",
    "- Load the Iris dataset and split it into training and test sets.\n",
    "- Convert the data into an XGBoost-compatible format using xgb.DMatrix\n",
    "- Define the parameters for the XGBoost model.\n",
    "- Train the model by specifying the training dataset, evaluation dataset, number of rounds and early stopping rounds\n",
    "- Plot the training loss using matplotlib\n",
    "\n",
    "Note that this is a basic example and you may want to tune the parameters and try different variations to get the best performance for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into an XGBoost-compatible format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Specify the learning task and the corresponding learning objective\n",
    "    'num_class': 3,  # Number of classes in the dataset\n",
    "    'tree_method': 'auto',  # Use the efficient tree building algorithm\n",
    "    'nthread': -1,  # Use all available CPU threads\n",
    "    'silent': 1,  # Don't print messages\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]  # Specify the datasets for evaluation during training\n",
    "num_round = 100  # Number of rounds (iterations) to run\n",
    "bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=10)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(bst.get_score(importance_type='gain'))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of using grid search to find the best hyperparameters for an XGBoost classifier on a built-in dataset from scikit-learn, and plotting the training loss:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code does the following:\n",
    "\n",
    "- Import the necessary libraries: xgboost, scikit-learn's load_iris function, train_test_split function, GridSearchCV and matplotlib for plotting\n",
    "- Load the Iris dataset and split it into training and test sets.\n",
    "- Convert the data into an XGBoost-compatible format using xgb.DMatrix\n",
    "- Define the parameters for the XGBoost model.\n",
    "- Run a grid search using the GridSearchCV function to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into an XGBoost-compatible format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "param_grid = {\n",
    "    'objective': ['multi:softmax'],  # Specify the learning task and the corresponding learning objective\n",
    "    'num_class': [3],  # Number of classes in the dataset\n",
    "    'tree_method': ['auto'],  # Use the efficient tree building algorithm\n",
    "    'nthread': [-1],  # Use all available CPU threads\n",
    "    'silent': [1],  # Don't print messages\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Run GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "params = grid_search.best_params_\n",
    "num_round = 100  # Number of rounds (iterations) to run\n",
    "bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=10)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(bst.get_score(importance_type='gain'))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
